Асинхронный веб-скраппер для сбора и очистки товаров с веб-сайта https://interlamp.by/.

Особенности:
1. Список прокси-серверов используемый в tuple proxies в функции get_random_proxy заполняется любым количеством работающих прокси-серверов по указанному шаблону
(При желании для удобства можно сделать, либо ручное наполнение через ввод из консоли, либо считывая из txt, csv или xlsx файлов. Однако будьте осторожны, так чтение файлов является блокирующей операцие и внесение таких изменений может повлечь отказ в работе скраппера. Во измбежание подобного предлагаю ознакомиться с документацией по этому адресу https://docs.python.org/3/library/asyncio-task.html#id10);

2. Количество страниц в каждом из двух каталогов и, как следствие, общее количество товаров подлежащих скраппингу определяется автоматически в функциях get_total_pages и get_total_pages_outdoor;

3. Во избежание блокирования в функции get_random_headers в tuple desktop_agents собраны наиболее распостраненные user-agents, которые рандомно подставляются в итоговый header. Если планируете использовать этот код как основу для написания скраппера для сайтов на языке отличном от русского, обратите внимание на accept-language в header;

4. При каждом обращении к сайту-донору происходит случайный и независимый друг от друга выбор user-agent и прокси-сервера, что при комбинировании дает довольно большое разнообразие и мешает серверу определить парсинг и заблокировать нас;

5. Результатом скраппинга является xlsx файл с определенным набором столбцов и без индексов. Таково требование заказчика, так как файл должен быть использован для автоматической загрузки и обновления информации об уже существующих на сайте товарах через установленный на сайте клиента модуль, который требует именно такой структуры. Для этого была использована библиотетка pandas, с помощью которой был создан data frame, а после из созданного data frame с использованием функции to_excel применной к нему был создан результирующий xlsx файл. При необходимости изменив функцию make_file_excel Вы можете изменить количество и название столбцов, листов, название файла и вернуть индексы или вообще получить csv или txt файл. Пример полученного файла также находится в проекте;

6. Ошибки в наполнении сайта-донор такие как неверная ссылака на товар, отсутствие статуса, отсутствие цены и так далее собираются в отдельный файл errors.txt, в котором указывается url страницы и тип ошибки. Таким образом собрав информацию сайта Вы можете улучшить качество сайта-акцептора. Например при скраппинге выяснилось, что на сайте-доноре отстутвовали в списке брендов два бренда, хотя товары этих брендов на сайте есть. Заказчик, будучи официальным дистрибьтером владельца сайта-донора получил важную информацию и добавил информацию об отствующих брендах на свой сайт, что положительно повлияло на seo сайта ;

7. Информация о технических ошибках таких как принудительный разрыв соединения сервером и прочих собираются в файл system_log.txt. Благодаря такому логированию можно отследить, например, не работающих прокси-сервер или ошибку в функции (ошибки обрабатываются с помощью try - except и в логе указывается в какой функции произошла ошибка).
